{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737865a6-fcbd-4850-87a9-25311751c8e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecfbeed7-162b-44b0-bd48-beed2be678d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\joshu\\anaconda3\\envs\\rag-env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os, pickle, requests, pdfplumber\n",
    "from dotenv import load_dotenv\n",
    "from langchain.schema import Document\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "import gradio as gr\n",
    "\n",
    "load_dotenv()\n",
    "API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "assert API_KEY, \"Please set OPENAI_API_KEY in your .env file\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e297c3-9c58-4c8d-a449-87ece066c91b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5dbf1aa-c77b-49dc-b738-af8a9827d824",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2952332-a9f6-456a-b60b-0d28d62de44f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ EMA guideline downloaded and converted to text in ./data/\n"
     ]
    }
   ],
   "source": [
    "# Cell 2 (simplified): Download one EMA PDF & convert to text\n",
    "\n",
    "import os, requests, pdfplumber\n",
    "\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "# Only EMA Bioanalytical Method Validation\n",
    "url = \"https://www.ema.europa.eu/en/documents/scientific-guideline/guideline-bioanalytical-method-validation_en.pdf\"\n",
    "r = requests.get(url, timeout=30)\n",
    "r.raise_for_status()\n",
    "pdf_path = \"data/ema_bioanalytical_guideline.pdf\"\n",
    "with open(pdf_path, \"wb\") as f:\n",
    "    f.write(r.content)\n",
    "\n",
    "# Convert EMA PDF -> .txt\n",
    "txt_path = pdf_path.replace(\".pdf\", \".txt\")\n",
    "pages = []\n",
    "with pdfplumber.open(pdf_path) as pdf:\n",
    "    for page in pdf.pages:\n",
    "        pages.append(page.extract_text() or \"\")\n",
    "with open(txt_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\\n\".join(pages))\n",
    "\n",
    "print(\"‚úÖ EMA guideline downloaded and converted to text in ./data/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374b7a81-8120-4bd6-8e04-7f560e2d84db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2728b932-d041-440c-a783-fd8bd64e4d45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1935cb6-a03b-487b-bda1-57dbd9c5b9f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Loaded 1 text docs for indexing.\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import Document\n",
    "import os\n",
    "\n",
    "def load_docs(folder=\"data\"):\n",
    "    docs = []\n",
    "    for fn in os.listdir(folder):\n",
    "        if not fn.endswith(\".txt\"):\n",
    "            continue\n",
    "        with open(os.path.join(folder, fn), encoding=\"utf-8\") as f:\n",
    "            txt = f.read()\n",
    "        docs.append(Document(page_content=txt, metadata={\"source\": fn}))\n",
    "    return docs\n",
    "\n",
    "all_docs = load_docs(\"data\")\n",
    "print(f\"üîç Loaded {len(all_docs)} text docs for indexing.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14b673e-2101-411f-9cb1-80dd6b98536c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbacd2c-04ba-4696-837d-2ef364837c95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42a5684e-59ba-412c-a01c-35b8460b8d25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\joshu\\AppData\\Local\\Temp\\ipykernel_2548\\3554573147.py:8: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedder = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ FAISS index built locally (no OpenAI calls) and saved to faiss_index.pkl\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Build & persist FAISS index with a local HF embedder\n",
    "!pip install --quiet sentence-transformers\n",
    "\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "import pickle\n",
    "\n",
    "embedder = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "faiss_index = FAISS.from_documents(all_docs, embedder)\n",
    "\n",
    "with open(\"faiss_index.pkl\", \"wb\") as f:\n",
    "    pickle.dump(faiss_index, f)\n",
    "\n",
    "print(\"‚úÖ FAISS index built locally (no OpenAI calls) and saved to faiss_index.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df40cdd8-a278-42ef-9cb6-7a8a01c71641",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2a2fa4-26f0-47ae-a9bb-36b5fef37341",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87341c08-1f3a-4636-b07d-352cce47c795",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7326d4-1aa7-4a0f-9571-cd4c479197fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4c3e82-6890-458d-b2b6-efab69f404d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ed157ed-e38c-489e-8fa2-7ad8d82f102e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\joshu\\AppData\\Local\\Temp\\ipykernel_2548\\774492991.py:10: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  llm = ChatOpenAI(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ RAG chain is ready!\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Wire up the RAG chain with GPT-3.5 using the v1.0+ API\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# (If you restarted, reload the FAISS store:)\n",
    "# with open(\"faiss_index.pkl\", \"rb\") as f:\n",
    "#     faiss_index = pickle.load(f)\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model_name=\"gpt-3.5-turbo\",\n",
    "    temperature=0,\n",
    "    openai_api_key=API_KEY\n",
    ")\n",
    "retriever = faiss_index.as_retriever(search_kwargs={\"k\": 5})\n",
    "\n",
    "rag = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "print(\"‚úÖ RAG chain is ready!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5302be2-55b1-4000-a9c7-1c3af630b1ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca58150-17f6-42d4-bb37-3f34997c311c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888eee65-fc03-4aab-920d-78c2238b0902",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ccb1915-73ff-495a-b1c5-0027e4971ea7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf5255c-0c66-4b12-bf8f-73bebded4a46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf64af64-b95e-4921-bdf1-b0469803d20c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e36ad41b-d862-4d0f-970b-c62342350a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* Running on public URL: https://237e8cdbc0a30f4a59.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://237e8cdbc0a30f4a59.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\joshu\\AppData\\Local\\Temp\\ipykernel_2548\\3418708132.py:2: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  res = rag({\"query\": question})\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\joshu\\anaconda3\\envs\\rag-env\\lib\\site-packages\\gradio\\queueing.py\", line 626, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "  File \"C:\\Users\\joshu\\anaconda3\\envs\\rag-env\\lib\\site-packages\\gradio\\route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"C:\\Users\\joshu\\anaconda3\\envs\\rag-env\\lib\\site-packages\\gradio\\blocks.py\", line 2220, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"C:\\Users\\joshu\\anaconda3\\envs\\rag-env\\lib\\site-packages\\gradio\\blocks.py\", line 1731, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "  File \"C:\\Users\\joshu\\anaconda3\\envs\\rag-env\\lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "  File \"C:\\Users\\joshu\\anaconda3\\envs\\rag-env\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2470, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"C:\\Users\\joshu\\anaconda3\\envs\\rag-env\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 967, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"C:\\Users\\joshu\\anaconda3\\envs\\rag-env\\lib\\site-packages\\gradio\\utils.py\", line 940, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"C:\\Users\\joshu\\AppData\\Local\\Temp\\ipykernel_2548\\3418708132.py\", line 2, in answer_with_sources\n",
      "    res = rag({\"query\": question})\n",
      "  File \"C:\\Users\\joshu\\anaconda3\\envs\\rag-env\\lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 189, in warning_emitting_wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "  File \"C:\\Users\\joshu\\anaconda3\\envs\\rag-env\\lib\\site-packages\\langchain\\chains\\base.py\", line 386, in __call__\n",
      "    return self.invoke(\n",
      "  File \"C:\\Users\\joshu\\anaconda3\\envs\\rag-env\\lib\\site-packages\\langchain\\chains\\base.py\", line 167, in invoke\n",
      "    raise e\n",
      "  File \"C:\\Users\\joshu\\anaconda3\\envs\\rag-env\\lib\\site-packages\\langchain\\chains\\base.py\", line 157, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"C:\\Users\\joshu\\anaconda3\\envs\\rag-env\\lib\\site-packages\\langchain\\chains\\retrieval_qa\\base.py\", line 154, in _call\n",
      "    answer = self.combine_documents_chain.run(\n",
      "  File \"C:\\Users\\joshu\\anaconda3\\envs\\rag-env\\lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 189, in warning_emitting_wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "  File \"C:\\Users\\joshu\\anaconda3\\envs\\rag-env\\lib\\site-packages\\langchain\\chains\\base.py\", line 608, in run\n",
      "    return self(kwargs, callbacks=callbacks, tags=tags, metadata=metadata)[\n",
      "  File \"C:\\Users\\joshu\\anaconda3\\envs\\rag-env\\lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 189, in warning_emitting_wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "  File \"C:\\Users\\joshu\\anaconda3\\envs\\rag-env\\lib\\site-packages\\langchain\\chains\\base.py\", line 386, in __call__\n",
      "    return self.invoke(\n",
      "  File \"C:\\Users\\joshu\\anaconda3\\envs\\rag-env\\lib\\site-packages\\langchain\\chains\\base.py\", line 167, in invoke\n",
      "    raise e\n",
      "  File \"C:\\Users\\joshu\\anaconda3\\envs\\rag-env\\lib\\site-packages\\langchain\\chains\\base.py\", line 157, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"C:\\Users\\joshu\\anaconda3\\envs\\rag-env\\lib\\site-packages\\langchain\\chains\\combine_documents\\base.py\", line 138, in _call\n",
      "    output, extra_return_dict = self.combine_docs(\n",
      "  File \"C:\\Users\\joshu\\anaconda3\\envs\\rag-env\\lib\\site-packages\\langchain\\chains\\combine_documents\\stuff.py\", line 259, in combine_docs\n",
      "    return self.llm_chain.predict(callbacks=callbacks, **inputs), {}\n",
      "  File \"C:\\Users\\joshu\\anaconda3\\envs\\rag-env\\lib\\site-packages\\langchain\\chains\\llm.py\", line 319, in predict\n",
      "    return self(kwargs, callbacks=callbacks)[self.output_key]\n",
      "  File \"C:\\Users\\joshu\\anaconda3\\envs\\rag-env\\lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 189, in warning_emitting_wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "  File \"C:\\Users\\joshu\\anaconda3\\envs\\rag-env\\lib\\site-packages\\langchain\\chains\\base.py\", line 386, in __call__\n",
      "    return self.invoke(\n",
      "  File \"C:\\Users\\joshu\\anaconda3\\envs\\rag-env\\lib\\site-packages\\langchain\\chains\\base.py\", line 167, in invoke\n",
      "    raise e\n",
      "  File \"C:\\Users\\joshu\\anaconda3\\envs\\rag-env\\lib\\site-packages\\langchain\\chains\\base.py\", line 157, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"C:\\Users\\joshu\\anaconda3\\envs\\rag-env\\lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
      "    response = self.generate([inputs], run_manager=run_manager)\n",
      "  File \"C:\\Users\\joshu\\anaconda3\\envs\\rag-env\\lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
      "    return self.llm.generate_prompt(\n",
      "  File \"C:\\Users\\joshu\\anaconda3\\envs\\rag-env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 963, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"C:\\Users\\joshu\\anaconda3\\envs\\rag-env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 782, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"C:\\Users\\joshu\\anaconda3\\envs\\rag-env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 1028, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"C:\\Users\\joshu\\anaconda3\\envs\\rag-env\\lib\\site-packages\\langchain_community\\chat_models\\openai.py\", line 476, in _generate\n",
      "    response = self.completion_with_retry(\n",
      "  File \"C:\\Users\\joshu\\anaconda3\\envs\\rag-env\\lib\\site-packages\\langchain_community\\chat_models\\openai.py\", line 387, in completion_with_retry\n",
      "    return self.client.create(**kwargs)\n",
      "  File \"C:\\Users\\joshu\\anaconda3\\envs\\rag-env\\lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\joshu\\anaconda3\\envs\\rag-env\\lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 1087, in create\n",
      "    return self._post(\n",
      "  File \"C:\\Users\\joshu\\anaconda3\\envs\\rag-env\\lib\\site-packages\\openai\\_base_client.py\", line 1249, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"C:\\Users\\joshu\\anaconda3\\envs\\rag-env\\lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n"
     ]
    }
   ],
   "source": [
    "def answer_with_sources(question):\n",
    "    res = rag({\"query\": question})\n",
    "    answer = res[\"result\"]\n",
    "    sources = \"\\n\".join(f\"- {d.metadata['source']}\" for d in res[\"source_documents\"])\n",
    "    return f\"**Answer:**\\n{answer}\\n\\n**Sources:**\\n{sources}\"\n",
    "\n",
    "iface = gr.Interface(\n",
    "    fn=answer_with_sources,\n",
    "    inputs=gr.Textbox(lines=2, placeholder=\"Ask medical questions‚Ä¶\"),\n",
    "    outputs=\"markdown\",\n",
    "    title=\"ü©∫ Medical RAG Demo\",\n",
    "    description=\"GPT-3.5 + EMA Bioanalytical Guideline\"\n",
    ")\n",
    "\n",
    "iface.launch(share=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ea76f3-66b0-4ebb-ab9d-c76209435020",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c58283-fd41-4ca2-84cc-2982183d9c64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60035c83-91e4-4a8f-b8e7-880bc751126e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
